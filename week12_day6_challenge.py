# -*- coding: utf-8 -*-
"""Week12/day6/challenge.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/12Oe_a_XqBPQUvTXYXFG96LPYtT_ZV_8j
"""

!pip install requests beautifulsoup4

!pip install selenium

!pip install selenium webdriver_manager

import time
import pandas as pd
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from bs4 import BeautifulSoup

# Setup Chrome options for headless browsing
chrome_options = Options()
chrome_options.add_argument("--headless")
chrome_options.add_argument("--no-sandbox")
chrome_options.add_argument("--disable-dev-shm-usage")

# Setup Selenium WebDriver with Chrome options
driver = webdriver.Chrome(options=chrome_options)

url = "https://github.com/topics"
driver.get(url)

# Function to scroll to the bottom of the page
def scroll_to_bottom():
    driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
    time.sleep(2)  # Wait for content to load

# Keep clicking "Load more..." until it's no longer available
while True:
    scroll_to_bottom()
    try:
        load_more_button = WebDriverWait(driver, 10).until(
            EC.element_to_be_clickable((By.XPATH, "//button[contains(text(), 'Load more')]"))
        )
        load_more_button.click()
        time.sleep(2)  # Wait for new content to load
    except:
        print("No more 'Load more' button found. All topics loaded.")
        break

# Get the page source after all topics are loaded
html_content = driver.page_source

# Parse the HTML content with BeautifulSoup
soup = BeautifulSoup(html_content, 'html.parser')

# Find all topic containers
topic_containers = soup.find_all('div', class_='py-4 border-bottom d-flex flex-justify-between')

# Create a list to store dictionaries of topic data
topics_data = []

# Extract topic titles and descriptions
for container in topic_containers:
    # Extract title
    title_element = container.find('p', class_='f3 lh-condensed mb-0 mt-1 Link--primary')
    title = title_element.text.strip() if title_element else "No title found"

    # Extract description
    desc_element = container.find('p', class_='f5 color-fg-muted mb-0 mt-1')
    description = desc_element.text.strip() if desc_element else "No description found"

    # Append data to the list as a dictionary
    topics_data.append({
        'title': title,
        'description': description
    })

# Close the browser
driver.quit()

# Convert the list of dictionaries to a pandas DataFrame
df = pd.DataFrame(topics_data)

# Print the DataFrame
print(df)