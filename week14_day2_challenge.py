# -*- coding: utf-8 -*-
"""Week14_day2_challenge.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1eR4m3LPuUdcvFEBiCL0bk6PPzVQBAZAv
"""

# Import necessary libraries
import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
from google.colab import files
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score

# Upload the file
uploaded = files.upload()

# Read the uploaded file
df = pd.read_csv('ex2data1.txt', header=None, names=['Exam 1', 'Exam 2', 'Admitted'])

# Examine the first few rows
print(df.head())

# Create a scatter plot
plt.figure(figsize=(10, 6))
admitted = df[df['Admitted'] == 1]
not_admitted = df[df['Admitted'] == 0]

plt.scatter(not_admitted['Exam 1'], not_admitted['Exam 2'], c='blue', label='Not Admitted')
plt.scatter(admitted['Exam 1'], admitted['Exam 2'], c='yellow', label='Admitted')

plt.xlabel('Exam 1 Score')
plt.ylabel('Exam 2 Score')
plt.title('University Admission based on Exam Scores')
plt.legend()
plt.grid(True)
plt.show()

# Prepare the data for logistic regression
X = df[['Exam 1', 'Exam 2']]
y = df['Admitted']

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Create and train the logistic regression model
model = LogisticRegression()
model.fit(X_train, y_train)

# Make predictions on the test set
y_pred = model.predict(X_test)

# Calculate the accuracy of the model
accuracy = accuracy_score(y_test, y_pred)
print(f"\nModel Accuracy: {accuracy:.2f}")

# Print the model coefficients
print("\nModel Coefficients:")
print(f"Exam 1: {model.coef_[0][0]:.4f}")
print(f"Exam 2: {model.coef_[0][1]:.4f}")
print(f"Intercept: {model.intercept_[0]:.4f}")

# Visualize the decision boundary
plt.figure(figsize=(10, 6))
plt.scatter(not_admitted['Exam 1'], not_admitted['Exam 2'], c='blue', label='Not Admitted')
plt.scatter(admitted['Exam 1'], admitted['Exam 2'], c='yellow', label='Admitted')

# Create a meshgrid to plot the decision boundary
x1 = X['Exam 1']
x2 = X['Exam 2']
x1_min, x1_max = x1.min() - 1, x1.max() + 1
x2_min, x2_max = x2.min() - 1, x2.max() + 1
xx1, xx2 = np.meshgrid(np.arange(x1_min, x1_max, 0.01),
                       np.arange(x2_min, x2_max, 0.01))
Z = model.predict(np.c_[xx1.ravel(), xx2.ravel()])
Z = Z.reshape(xx1.shape)

plt.contour(xx1, xx2, Z, alpha=0.8, cmap=plt.cm.RdYlBu)

plt.xlabel('Exam 1 Score')
plt.ylabel('Exam 2 Score')
plt.title('Logistic Regression Decision Boundary')
plt.legend()
plt.grid(True)
plt.show()

"""**Model Accuracy:**
The model accuracy is 0.80. This means that the model correctly predicts the admission status for 80% of the students in the test set. This is a good accuracy for a simple logistic regression model, indicating that the exam scores are indeed predictive of admission status.
Model Coefficients:

Exam 1: 0.2597
Exam 2: 0.2248
Intercept: -30.1998



**Interpretation:**

Both exam scores have positive coefficients, which means higher scores in either exam increase the likelihood of admission.
Exam 1 has a slightly higher coefficient (0.2597) compared to Exam 2 (0.2248), suggesting that Exam 1 might be slightly more important in determining admission.
The intercept is negative (-30.1998), which means that with very low scores on both exams, the model predicts a low probability of admission.
"""