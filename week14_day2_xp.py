# -*- coding: utf-8 -*-
"""Week14_day2_xp.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1AQ_LPCsaQIkgBp-xU8NlZsyuiQOcLpiy
"""

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# Load the dataset
df = pd.read_csv('/content/diabetes_prediction_dataset.csv')

# Display basic information about the dataset
print(df.info())

# Show the first few rows
print(df.head())

# Display summary statistics
print(df.describe())

# Check for missing values
print(df.isnull().sum())

# Display unique values in categorical columns
categorical_columns = ['gender', 'smoking_history']
for col in categorical_columns:
    print(f"\nUnique values in {col}:")
    print(df[col].value_counts())

# Plot distribution of numeric features
numeric_columns = ['age', 'bmi', 'HbA1c_level', 'blood_glucose_level']
fig, axes = plt.subplots(2, 2, figsize=(15, 10))
for i, col in enumerate(numeric_columns):
    sns.histplot(df[col], ax=axes[i//2, i%2], kde=True)
    axes[i//2, i%2].set_title(f'Distribution of {col}')
plt.tight_layout()
plt.show()

# Plot correlation heatmap
plt.figure(figsize=(10, 8))
sns.heatmap(df.corr(), annot=True, cmap='coolwarm')
plt.title('Correlation Heatmap')
plt.show()

# Check class balance
print("\nClass balance:")
print(df['diabetes'].value_counts(normalize=True))

# Plot diabetes prevalence by gender
plt.figure(figsize=(8, 6))
sns.countplot(x='gender', hue='diabetes', data=df)
plt.title('Diabetes Prevalence by Gender')
plt.show()

# Plot diabetes prevalence by smoking history
plt.figure(figsize=(12, 6))
sns.countplot(x='smoking_history', hue='diabetes', data=df)
plt.title('Diabetes Prevalence by Smoking History')
plt.xticks(rotation=45)
plt.show()

# Box plot of age vs diabetes
plt.figure(figsize=(8, 6))
sns.boxplot(x='diabetes', y='age', data=df)
plt.title('Age Distribution by Diabetes Status')
plt.show()

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import StandardScaler


# Encode categorical variables
df_encoded = pd.get_dummies(df, columns=['gender', 'smoking_history'])

# Create age groups
df_encoded['age_group'] = pd.cut(df_encoded['age'], bins=[0, 18, 40, 60, 100], labels=['Child', 'Young Adult', 'Adult', 'Senior'])

# Create BMI categories
df_encoded['bmi_category'] = pd.cut(df_encoded['bmi'], bins=[0, 18.5, 25, 30, 100], labels=['Underweight', 'Normal', 'Overweight', 'Obese'])

# Scale numerical features
scaler = StandardScaler()
numerical_features = ['age', 'bmi', 'HbA1c_level', 'blood_glucose_level']
df_encoded[numerical_features] = scaler.fit_transform(df_encoded[numerical_features])

# Plot correlation heatmap for numerical features
plt.figure(figsize=(10, 8))
sns.heatmap(df_encoded[numerical_features + ['diabetes']].corr(), annot=True, cmap='coolwarm')
plt.title('Correlation Heatmap (Numerical Features)')
plt.show()

# Check class balance
print("\nClass balance:")
print(df_encoded['diabetes'].value_counts(normalize=True))

# Display the first few rows of the prepared dataset
print(df_encoded.head())

# Summary of the prepared dataset
print(df_encoded.info())

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import confusion_matrix, classification_report, roc_curve, auc, accuracy_score
from sklearn.preprocessing import StandardScaler
from imblearn.over_sampling import SMOTE

# Select a subset of important features (adjust based on your domain knowledge)
important_features = ['age', 'bmi', 'HbA1c_level', 'blood_glucose_level', 'hypertension', 'heart_disease']
X = df_encoded[important_features]
y = df_encoded['diabetes']

# Split the data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Apply SMOTE to handle class imbalance
smote = SMOTE(random_state=42)
X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)

# Scale the features
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train_resampled)
X_test_scaled = scaler.transform(X_test)

# Train the logistic regression model
model = LogisticRegression(random_state=42, max_iter=1000)
model.fit(X_train_scaled, y_train_resampled)

# Make predictions
y_pred = model.predict(X_test_scaled)
y_pred_proba = model.predict_proba(X_test_scaled)[:, 1]

# Calculate accuracy
accuracy = accuracy_score(y_test, y_pred)

# 1. Plot the accuracy score
plt.figure(figsize=(8, 6))
plt.bar(['Accuracy'], [accuracy])
plt.title('Accuracy Score of Logistic Regression')
plt.ylim(0, 1)
plt.text(0, accuracy, f'{accuracy:.3f}', ha='center', va='bottom')
plt.show()

print(f"Accuracy: {accuracy:.3f}")

# 2. Plot the confusion matrix
cm = confusion_matrix(y_test, y_pred)
plt.figure(figsize=(8, 6))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')
plt.title('Confusion Matrix')
plt.ylabel('True label')
plt.xlabel('Predicted label')
plt.show()

print("\nConfusion Matrix:")
print(cm)

# 3. Plot Recall, Precision, F1-score
report = classification_report(y_test, y_pred, output_dict=True)
metrics = ['precision', 'recall', 'f1-score']
values = [report['1'][metric] for metric in metrics]

plt.figure(figsize=(10, 6))
plt.bar(metrics, values)
plt.title('Precision, Recall, and F1-score for Diabetes (Class 1)')
plt.ylim(0, 1)
for i, v in enumerate(values):
    plt.text(i, v, f'{v:.3f}', ha='center', va='bottom')
plt.show()

print("\nClassification Report:")
print(classification_report(y_test, y_pred))

# 4. Visualize the decision boundary (using first two features)
plt.figure(figsize=(10, 8))
x0, x1 = X_test_scaled[:, 0], X_test_scaled[:, 1]
xx, yy = np.meshgrid(np.linspace(x0.min(), x0.max(), 100),
                     np.linspace(x1.min(), x1.max(), 100))
Z = model.predict_proba(np.c_[xx.ravel(), yy.ravel(), np.zeros_like(xx.ravel()),
                              np.zeros_like(xx.ravel()), np.zeros_like(xx.ravel()),
                              np.zeros_like(xx.ravel())])[:, 1]
Z = Z.reshape(xx.shape)
plt.contourf(xx, yy, Z, alpha=0.8, cmap=plt.cm.RdYlBu)
plt.scatter(x0, x1, c=y_test, cmap=plt.cm.RdYlBu, edgecolor='black')
plt.title(f'Decision Boundary (First Two Features)\nAccuracy: {accuracy:.3f}')
plt.xlabel(important_features[0])
plt.ylabel(important_features[1])
plt.colorbar()
plt.show()

# 5. Plot the ROC Curve
fpr, tpr, _ = roc_curve(y_test, y_pred_proba)
roc_auc = auc(fpr, tpr)

plt.figure(figsize=(8, 6))
plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (AUC = {roc_auc:.2f})')
plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic (ROC) Curve')
plt.legend(loc="lower right")
plt.show()

print(f"\nROC AUC: {roc_auc:.3f}")