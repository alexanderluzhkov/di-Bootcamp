# -*- coding: utf-8 -*-
"""Week14_day4_challenge.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1MgCwYrytXAEbXni2vplD75ocAoinr5rI
"""

# Import necessary libraries
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.neighbors import KNeighborsClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score

# Load the dataset
data = pd.read_csv('/content/data.csv')

# Exploratory Data Analysis
print("First few rows of the dataset:")
print(data.head())

# Check for missing values
print("\nMissing values:")
print(data.isnull().sum())

# Drop unnecessary columns
data = data.drop(['id', 'Unnamed: 32'], axis=1)

# Create a countplot to display diagnosis
plt.figure(figsize=(8, 6))
sns.countplot(x='diagnosis', data=data, palette='magma')
plt.title('Count of Diagnosis')
plt.show()

# Data Preprocessing
# Count unique values in 'diagnosis' column
print("\nCounts of unique values in 'diagnosis' column:")
print(data['diagnosis'].value_counts())

# Map categorical values to numerical values
data['diagnosis'] = data['diagnosis'].map({'M': 1, 'B': 0})

# Splitting features and target
X = data.drop('diagnosis', axis=1)
y = data['diagnosis']

# Check for any remaining NaN values
print("\nRemaining NaN values:")
print(X.isna().sum())

# Splitting the data into train and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Feature scaling
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Function to train and evaluate models
def train_evaluate_model(model, X_train, X_test, y_train, y_test):
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)
    accuracy = accuracy_score(y_test, y_pred)
    return accuracy

# Logistic Regression
lr_model = LogisticRegression(random_state=42)
lr_accuracy = train_evaluate_model(lr_model, X_train_scaled, X_test_scaled, y_train, y_test)
print(f"\nLogistic Regression Accuracy: {lr_accuracy:.4f}")

# K Nearest Neighbours
knn_model = KNeighborsClassifier()
knn_accuracy = train_evaluate_model(knn_model, X_train_scaled, X_test_scaled, y_train, y_test)
print(f"K Nearest Neighbours Accuracy: {knn_accuracy:.4f}")

# Random Forests
rf_model = RandomForestClassifier(random_state=42)
rf_accuracy = train_evaluate_model(rf_model, X_train_scaled, X_test_scaled, y_train, y_test)
print(f"Random Forests Accuracy: {rf_accuracy:.4f}")

# Support Vector Machines (SVM)
svm_model = SVC(random_state=42)
svm_accuracy = train_evaluate_model(svm_model, X_train_scaled, X_test_scaled, y_train, y_test)
print(f"Support Vector Machines Accuracy: {svm_accuracy:.4f}")

# Import necessary libraries
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.neighbors import KNeighborsClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score, classification_report

# Load the dataset
data = pd.read_csv('/content/data.csv')

# Drop unnecessary columns
data = data.drop(['id', 'Unnamed: 32'], axis=1)

# Map categorical values to numerical values
data['diagnosis'] = data['diagnosis'].map({'M': 1, 'B': 0})

# Splitting features and target
X = data.drop('diagnosis', axis=1)
y = data['diagnosis']

# Splitting the data into train and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Feature scaling
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Function to train and evaluate models
def train_evaluate_model(model, X_train, X_test, y_train, y_test):
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)
    accuracy = accuracy_score(y_test, y_pred)
    report = classification_report(y_test, y_pred, target_names=['Benign', 'Malignant'])
    return accuracy, report

# List of models
models = [
    ('Logistic Regression', LogisticRegression(random_state=42)),
    ('K Nearest Neighbours', KNeighborsClassifier()),
    ('Random Forests', RandomForestClassifier(random_state=42)),
    ('Support Vector Machines', SVC(random_state=42))
]

# Evaluate each model
for name, model in models:
    accuracy, report = train_evaluate_model(model, X_train_scaled, X_test_scaled, y_train, y_test)
    print(f"\n{name}:")
    print(f"Accuracy: {accuracy:.4f}")
    print("Classification Report:")
    print(report)

"""In a medical context like breast cancer diagnosis, high recall for Malignant cases is crucial to minimize false negatives. In this regard, Logistic Regression and SVM perform the best with 0.95 recall for Malignant cases.

"""